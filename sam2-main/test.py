import os
import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm
from pycocotools.coco import COCO
import clip
import random
import pickle
import torchvision
import textwrap

# ================= ğŸ”§ è·¯å¾„é…ç½® =================
# COCO æ ‡æ³¨
ANN_PATH = r"D:\Data\refcoco\annotations_trainval2014\annotations\instances_train2014.json"
# å›¾ç‰‡æ ¹ç›®å½•
IMG_BASE_DIR = r"D:\Data\refcoco\train2014\train2014"
# RefCOCOg æ•°æ®ç›®å½•
REF_DATA_DIR = r"D:\Data\refcoco\refcocog"
REF_FILE_NAME = "refs(google).p"

# ç»“æœä¿å­˜è·¯å¾„
SAVE_DIR = "output/vis_results"
os.makedirs(SAVE_DIR, exist_ok=True)
# ===============================================

# --- 1. NMS å®‰å…¨è¡¥ä¸ ---
try:
    from torchvision.ops import nms as _orig_nms


    def safe_nms(boxes, scores, iou_threshold):
        return _orig_nms(boxes.cpu(), scores.cpu(), iou_threshold).to(boxes.device)


    torchvision.ops.nms = safe_nms
    print("ğŸ›¡ï¸ NMS å®‰å…¨è¡¥ä¸å·²æ¿€æ´»")
except:
    pass

# --- 2. æ¨¡å‹åˆå§‹åŒ– ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ åˆå§‹åŒ–è®¾å¤‡: {device}")

model_clip, preprocess = clip.load("ViT-B/32", device=device)

from sam2.build_sam import build_sam2
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

sam2_checkpoint = r"./checkpoints/sam2.1_hiera_base_plus.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_b+.yaml"
sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)
mask_generator = SAM2AutomaticMaskGenerator(sam2_model)


# --- 3. æ•°æ®åŠ è½½ ---
def load_ref_data():
    path = os.path.join(REF_DATA_DIR, REF_FILE_NAME)
    if not os.path.exists(path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {path}")
        return None
    with open(path, 'rb') as f:
        refs = pickle.load(f)
    # è¿‡æ»¤éªŒè¯é›†
    val_refs = [r for r in refs if r['split'] == 'val']
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: å…± {len(val_refs)} æ¡éªŒè¯é›†æ•°æ®")
    return val_refs


def resolve_target(coco, ref_item):
    # --- æ ¸å¿ƒä¿®å¤é€»è¾‘ ---
    # ä¸è¦ç”¨ ref_item['file_name']ï¼Œå› ä¸ºå®ƒå¸¦æœ‰ _annID åç¼€
    # æˆ‘ä»¬ç›´æ¥ç”¨ image_id é‡æ„æ ‡å‡†æ–‡ä»¶å
    img_id = ref_item['image_id']
    fname = f"COCO_train2014_{img_id:012d}.jpg"

    img_path = os.path.join(IMG_BASE_DIR, fname)
    if not os.path.exists(img_path):
        # å°è¯•å»ä¸Šä¸€çº§ç›®å½•æ‰¾ (é˜²æ­¢ç›®å½•ç»“æ„å·®å¼‚)
        img_path = os.path.join(os.path.dirname(IMG_BASE_DIR), fname)

    if not os.path.exists(img_path):
        # print(f"æ‰¾ä¸åˆ°å›¾ç‰‡: {img_path}") # è°ƒè¯•ç”¨
        return None, None

    # è·å–çœŸå€¼ Mask
    try:
        ann_id = ref_item['ann_id']
        ann = coco.loadAnns(ann_id)[0]
        gt_mask = coco.annToMask(ann)
        return img_path, gt_mask
    except Exception:
        return None, None


# --- 4. ä¸»å¾ªç¯ ---
def run_main(num_samples=20):
    print("â³ åˆå§‹åŒ– COCO API...")
    coco = COCO(ANN_PATH)

    refs = load_ref_data()
    if refs is None: return

    sampled_refs = random.sample(refs, min(num_samples * 2, len(refs)))
    results = []
    processed_count = 0

    print(f"ğŸ å¼€å§‹æµ‹è¯• (ç›®æ ‡: {num_samples} å¼ )...")
    pbar = tqdm(total=num_samples)

    for ref in sampled_refs:
        if processed_count >= num_samples: break

        # æå–æ–‡æœ¬
        if 'sentences' not in ref or not ref['sentences']: continue
        text_query = ref['sentences'][0]['sent']

        # è·å–è·¯å¾„å’ŒçœŸå€¼
        img_path, gt_mask = resolve_target(coco, ref)
        if img_path is None: continue

        try:
            image = Image.open(img_path).convert("RGB")
            img_np = np.array(image)

            # === SAM2 ===
            with torch.inference_mode(), torch.autocast(device, dtype=torch.bfloat16):
                masks = mask_generator.generate(img_np)
            if not masks: continue

            # === CLIP ===
            text_token = clip.tokenize([text_query[:77]]).to(device)
            with torch.no_grad():
                text_feat = model_clip.encode_text(text_token)
                text_feat /= text_feat.norm(dim=-1, keepdim=True)

                scores = []
                for m in masks:
                    x, y, w, h = [int(v) for v in m['bbox']]
                    pad = 15
                    crop = image.crop((max(0, x - pad), max(0, y - pad), min(image.width, x + w + pad),
                                       min(image.height, y + h + pad)))
                    img_in = preprocess(crop).unsqueeze(0).to(device)
                    img_feat = model_clip.encode_image(img_in)
                    img_feat /= img_feat.norm(dim=-1, keepdim=True)
                    scores.append((img_feat @ text_feat.T).item())

            best_idx = np.argmax(scores)
            pred_mask = masks[best_idx]['segmentation']

            # === æŒ‡æ ‡ ===
            inter = np.logical_and(pred_mask, gt_mask).sum()
            union = np.logical_or(pred_mask, gt_mask).sum()
            iou = inter / union if union > 0 else 0
            results.append(iou)

            # === å¯è§†åŒ– ===
            plt.figure(figsize=(12, 7))

            # å·¦å›¾
            plt.subplot(1, 2, 1)
            plt.imshow(img_np)
            color_pred = np.array([1.0, 0.0, 0.0, 0.65])
            h, w = pred_mask.shape[-2:]
            mask_vis_pred = pred_mask.reshape(h, w, 1) * color_pred.reshape(1, 1, -1)
            plt.imshow(mask_vis_pred)

            y_ind, x_ind = np.where(pred_mask > 0)
            if len(y_ind) > 0:
                rect = plt.Rectangle((x_ind.min(), y_ind.min()), x_ind.max() - x_ind.min(), y_ind.max() - y_ind.min(),
                                     linewidth=2, edgecolor='yellow', facecolor='none')
                plt.gca().add_patch(rect)

            wrapped_text = "\n".join(textwrap.wrap(text_query, width=40))
            hit_status = "âœ… HIT" if iou > 0.5 else "âŒ MISS"

            plt.title(f"Prompt:\n{wrapped_text}\n\nIoU: {iou:.2f} | {hit_status}",
                      fontsize=11, fontweight='bold', color='blue', loc='left')
            plt.axis('off')

            # å³å›¾
            plt.subplot(1, 2, 2)
            plt.imshow(img_np)
            color_gt = np.array([0.0, 1.0, 0.0, 0.5])
            mask_vis_gt = gt_mask.reshape(h, w, 1) * color_gt.reshape(1, 1, -1)
            plt.imshow(mask_vis_gt)
            plt.title("Ground Truth", fontsize=12)
            plt.axis('off')

            save_path = f"{SAVE_DIR}/vis_{processed_count}.jpg"
            plt.savefig(save_path, bbox_inches='tight', dpi=150)
            plt.close()

            processed_count += 1
            pbar.update(1)
            pbar.set_description(f"IoU: {iou:.2f}")

        except Exception as e:
            continue

    pbar.close()
    if results:
        print(f"\nğŸ“Š æœ€ç»ˆæˆç»© (Device: {device})")
        print(f"âœ… mIoU: {np.mean(results):.4f}")
        print(f"ğŸ¯ æˆåŠŸç‡: {np.mean(np.array(results) > 0.5):.2%}")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(SAVE_DIR)}")


if __name__ == "__main__":
    run_main(20)
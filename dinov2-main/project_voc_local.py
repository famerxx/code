import warnings

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import sys
import xml.etree.ElementTree as ET
from tqdm import tqdm
import matplotlib.pyplot as plt
import time
import numpy as np

# ================= âš™ï¸ è·¯å¾„ä¸å‚æ•°é…ç½® =================
local_repo_path = r"D:\APP\computerView\dinov2-main"
# æƒé‡æ–‡ä»¶
local_weights_path = os.path.join(local_repo_path, "dinov2_vits14_pretrain.pth")
# æ•°æ®é›†
voc_root = r"D:\Data\VOCdevkit\VOC2012"
# æ‰¹æ¬¡å¤§å°
BATCH_SIZE = 32
# 224*224 å°ºå¯¸
IMG_SIZE = 224
EPOCHS = 1
LEARNING_RATE = 0.001
NUM_WORKERS = 4

VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair',
    'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',
    'sheep', 'sofa', 'train', 'tvmonitor'
]
class_to_idx = {cls_name: i for i, cls_name in enumerate(VOC_CLASSES)}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ================= ğŸ“‚ æ•°æ®é›†å®šä¹‰ =================
class VOCRealDataset(Dataset):
    def __init__(self, root_dir, transform=None, is_train=True):
        self.root = root_dir
        self.transform = transform
        self.img_dir = os.path.join(root_dir, "JPEGImages")
        self.ann_dir = os.path.join(root_dir, "Annotations")
        txt_name = "train.txt" if is_train else "val.txt"
        txt_path = os.path.join(root_dir, "ImageSets", "Main", txt_name)

        self.ids = []
        if os.path.exists(txt_path):
            with open(txt_path, "r") as f:
                self.ids = [line.strip() for line in f.readlines()]
        else:
            all_files = [f[:-4] for f in os.listdir(self.img_dir) if f.endswith('.jpg')]
            split = int(len(all_files) * 0.8)
            self.ids = all_files[:split] if is_train else all_files[split:]

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img_path = os.path.join(self.img_dir, f"{img_id}.jpg")
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE))

        ann_path = os.path.join(self.ann_dir, f"{img_id}.xml")
        target_class = 0
        if os.path.exists(ann_path):
            try:
                tree = ET.parse(ann_path)
                root = tree.getroot()
                for obj in root.findall('object'):
                    name = obj.find('name').text
                    if name in class_to_idx:
                        target_class = class_to_idx[name]
                        break
            except:
                pass

        if self.transform: image = self.transform(image)
        return image, target_class


# ================= ğŸ§  æ¨¡å‹å®šä¹‰ =================
class DINOv2LocalModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.printed_shapes = False

    def load_weights(self):
        print(f"\nğŸ“¦ æ­£åœ¨åŠ è½½æœ¬åœ° DINOv2...", flush=True)
        if not os.path.exists(os.path.join(local_repo_path, 'hubconf.py')):
            print(f"âŒ é”™è¯¯ï¼šåœ¨ {local_repo_path} ä¸‹æ‰¾ä¸åˆ° hubconf.py")
            sys.exit(1)
        if not os.path.exists(local_weights_path):
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ {local_weights_path}")
            sys.exit(1)

        self.backbone = torch.hub.load(local_repo_path, 'dinov2_vits14', source='local', pretrained=False)
        state_dict = torch.load(local_weights_path, map_location='cpu')
        self.backbone.load_state_dict(state_dict)
        print("   âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼(ç¦»çº¿æ¨¡å¼)", flush=True)

        for param in self.backbone.parameters():
            param.requires_grad = False

        self.feat_dim = 384
        self.classifier = nn.Linear(self.feat_dim, len(VOC_CLASSES)).to(device)
        self.flatten = nn.Flatten()

    def forward(self, x):
        with torch.no_grad():
            output = self.backbone.forward_features(x)
            global_feat = output["x_norm_clstoken"]
            dense_feat = output["x_norm_patchtokens"]

            if not self.printed_shapes:
                print("\n" + "=" * 50, flush=True)
                print("   ğŸ” [DINOv2 ç‰¹å¾æå–éªŒè¯]", flush=True)
                print(f"   è¾“å…¥å›¾åƒ Batch : {x.shape} (B, C, H, W)", flush=True)
                print(f"   Global Feature : {global_feat.shape} (B, 384) -> ç”¨äºåˆ†ç±»", flush=True)
                print(f"   Dense Feature  : {dense_feat.shape} (B, 256, 384) -> å±€éƒ¨ç»†èŠ‚", flush=True)
                print("=" * 50 + "\n", flush=True)
                self.printed_shapes = True

        x = self.flatten(global_feat)
        return self.classifier(x)


# ================= ğŸ“Š ç»˜å›¾å‡½æ•° 1: è®­ç»ƒæ›²çº¿ =================
def plot_training_curve(history):
    epochs = range(1, len(history['train_loss']) + 1)
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['train_loss'], 'b-o', label='Train Loss')
    plt.title('Training Loss');
    plt.xlabel('Epochs');
    plt.ylabel('Loss');
    plt.grid(True);
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['train_acc'], 'r-o', label='Train Acc')
    plt.plot(epochs, history['val_acc'], 'g-s', label='Val Acc')
    plt.title('Training & Validation Accuracy');
    plt.xlabel('Epochs');
    plt.ylabel('Accuracy (%)');
    plt.grid(True);
    plt.legend()
    plt.tight_layout()
    save_path = "output/training_curve.png"
    plt.savefig(save_path)
    print(f"\nğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º: {save_path}", flush=True)
    # plt.show() # å¦‚æœä¸æƒ³å¼¹çª—å¯ä»¥æ³¨é‡Šæ‰


# ================= ğŸ¨ ç»˜å›¾å‡½æ•° 2: éšæœºé¢„æµ‹å¯è§†åŒ– (æ–°åŠŸèƒ½) =================
def visualize_predictions(model, dataset, device, num_samples=20):
    print(f"\nğŸ¨ æ­£åœ¨æŠ½å– {num_samples} å¼ å›¾ç‰‡è¿›è¡Œå¯è§†åŒ–æµ‹è¯•...", flush=True)
    model.eval()

    # éšæœºæŠ½å–ç´¢å¼•
    indices = torch.randperm(len(dataset))[:num_samples].tolist()

    # è®¾ç½®ç”»å¸ƒ (4è¡Œ5åˆ—)
    fig, axes = plt.subplots(4, 5, figsize=(16, 12))
    fig.suptitle(f'Random {num_samples} Predictions (Green=Correct, Red=Wrong)', fontsize=16)

    # å›¾åƒåå½’ä¸€åŒ–å‚æ•° (ImageNet Standard)
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.224])

    for idx, ax in zip(indices, axes.flat):
        image, label = dataset[idx]

        # é¢„æµ‹
        input_tensor = image.unsqueeze(0).to(device)  # (1, 3, 224, 224)
        with torch.no_grad():
            output = model(input_tensor)
            pred = output.argmax(dim=1).item()

        # å¤„ç†å›¾ç‰‡ç”¨äºæ˜¾ç¤º (Tensor -> Numpy -> åå½’ä¸€åŒ–)
        img_display = image.permute(1, 2, 0).cpu().numpy()
        img_display = std * img_display + mean
        img_display = np.clip(img_display, 0, 1)  # é™åˆ¶åœ¨ 0-1 ä¹‹é—´

        # æ˜¾ç¤ºå›¾ç‰‡
        ax.imshow(img_display)

        # è®¾ç½®æ ‡é¢˜é¢œè‰²
        color = 'green' if pred == label else 'red'
        title_text = f"P: {VOC_CLASSES[pred]}\nT: {VOC_CLASSES[label]}"
        ax.set_title(title_text, color=color, fontsize=11, fontweight='bold')
        ax.axis('off')

    plt.tight_layout()
    save_path = "output/prediction_gallery.png"
    plt.savefig(save_path)
    print(f"ğŸ“¸ é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜ä¸º: {save_path}", flush=True)
    plt.show()


# ================= ğŸš€ ä¸»ç¨‹åº =================
def main():
    print(f"ğŸš€ è¿è¡Œè®¾å¤‡: {device} | çº¿ç¨‹æ•°: {NUM_WORKERS}", flush=True)

    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.224]),
    ])

    transform_val = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.224]),
    ])

    print("\nğŸ“‚ åŠ è½½æ•°æ®...", flush=True)
    train_set = VOCRealDataset(voc_root, transform=transform, is_train=True)
    val_set = VOCRealDataset(voc_root, transform=transform_val, is_train=False)

    print(f"   --------------------------------", flush=True)
    print(f"   ğŸ–¼ï¸  è®­ç»ƒé›†: {len(train_set)} å¼ ", flush=True)
    print(f"   ğŸ–¼ï¸  éªŒè¯é›†: {len(val_set)} å¼ ", flush=True)
    print(f"   ğŸ·ï¸  ç±»åˆ«æ•°: {len(VOC_CLASSES)} ç±»", flush=True)
    print(f"   --------------------------------", flush=True)

    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False,
                            num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True)

    model = DINOv2LocalModel(len(VOC_CLASSES))
    model.load_weights()
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.classifier.parameters(), lr=LEARNING_RATE, momentum=0.9)

    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}

    print("âš¡ æ­£åœ¨æ‰§è¡Œç‰¹å¾æå–æ£€æŸ¥ (Dummy Pass)...", flush=True)
    with torch.no_grad():
        dummy_input = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE).to(device)
        model(dummy_input)

    time.sleep(1.0)

    print(f"âš¡ å¼€å§‹è®­ç»ƒ (å…± {EPOCHS} è½®, {NUM_WORKERS} çº¿ç¨‹)...", flush=True)
    time.sleep(0.5)

    for epoch in range(EPOCHS):
        print(f"\n[ Epoch {epoch + 1}/{EPOCHS} ]", flush=True)
        time.sleep(0.2)

        # --- Train ---
        model.train()
        running_loss = 0.0;
        train_correct = 0;
        train_total = 0

        with tqdm(train_loader, desc="Train", unit="batch", ncols=100, leave=True, file=sys.stdout) as train_bar:
            for images, labels in train_bar:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                running_loss += loss.item() * images.size(0)
                _, predicted = outputs.max(1)
                train_total += labels.size(0)
                train_correct += predicted.eq(labels).sum().item()
                train_bar.set_postfix(
                    {"acc": f"{100. * train_correct / train_total:.1f}%", "loss": f"{loss.item():.2f}"})

        epoch_loss = running_loss / train_total
        epoch_acc = 100. * train_correct / train_total
        history['train_loss'].append(epoch_loss)
        history['train_acc'].append(epoch_acc)

        # --- Val ---
        model.eval()
        val_correct = 0;
        val_total = 0

        with tqdm(val_loader, desc="Val  ", unit="batch", ncols=100, leave=True, file=sys.stdout) as val_bar:
            with torch.no_grad():
                for images, labels in val_bar:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    _, predicted = outputs.max(1)
                    val_total += labels.size(0)
                    val_correct += predicted.eq(labels).sum().item()
                    val_bar.set_postfix({"acc": f"{100. * val_correct / val_total:.1f}%"})

        val_acc = 100. * val_correct / val_total
        history['val_acc'].append(val_acc)

        print("", flush=True)
        time.sleep(0.05)
        print(f"ğŸ“Š ç»“æœ: Train Acc: {epoch_acc:.2f}% | Val Acc: {val_acc:.2f}%", flush=True)
        sys.stdout.flush()
        time.sleep(0.5)

    print("\nâœ… è®­ç»ƒå®Œæˆï¼æ­£åœ¨ç”Ÿæˆå›¾è¡¨...", flush=True)
    plot_training_curve(history)

    # âœ…âœ…âœ… è¿™é‡Œçš„è°ƒç”¨æ˜¯æ–°å¢çš„ âœ…âœ…âœ…
    # åœ¨æœ€åè°ƒç”¨éšæœºå¯è§†åŒ–å‡½æ•°
    visualize_predictions(model, val_set, device, num_samples=20)


if __name__ == '__main__':
    torch.multiprocessing.freeze_support()
    main()